{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> \n",
    "# Spark with categorical features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warning\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding categorical features example\n",
    "\n",
    "Often we have categorical features with values given as strings which we would like to transform to numerical values. The analogue of sklearn's `LabelEncoder` is the `StringIndexer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_1 = sqlContext.createDataFrame([\n",
    "    (4, \"high\"),\n",
    "    (5, \"low\"),\n",
    "    (6, \"high\"),\n",
    "    (7, \"high\"),\n",
    "    (8,'medium')\n",
    "], [\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer(\n",
    "        inputCol='label',\n",
    "        outputCol='label' + \"_index\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id| label|label_index|\n",
      "+---+------+-----------+\n",
      "|  4|  high|        0.0|\n",
      "|  5|   low|        1.0|\n",
      "|  6|  high|        0.0|\n",
      "|  7|  high|        0.0|\n",
      "|  8|medium|        2.0|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_2 = string_indexer.fit(ex_1).transform(ex_1)\n",
    "ex_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoderEstimator(\n",
    "        dropLast=True,\n",
    "        inputCols=['label_index'],\n",
    "        outputCols=['label' + \"_index_1\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------+\n",
      "| id| label|label_index|label_index_1|\n",
      "+---+------+-----------+-------------+\n",
      "|  4|  high|        0.0|(2,[0],[1.0])|\n",
      "|  5|   low|        1.0|(2,[1],[1.0])|\n",
      "|  6|  high|        0.0|(2,[0],[1.0])|\n",
      "|  7|  high|        0.0|(2,[0],[1.0])|\n",
      "|  8|medium|        2.0|    (2,[],[])|\n",
      "+---+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot.fit(ex_2).transform(ex_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot-encoded values are given as a sparse vector for each observation. The first number indicates the length of the sparse vector, the second number in brackets indicates the position that is filled with the last value. As you can see from the last shown entry, dropping a redundant label (`drop_last`) is default here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the car evaluation dataset \n",
    "\n",
    "```python\n",
    "df = pd.read_csv('../../../../resource-datasets/car_evaluation/car.csv')\n",
    "```\n",
    "\n",
    "Use `acceptability` as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../resource-datasets/car_evaluation/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(buying='vhigh', maint='vhigh', doors='2', persons='2', lug_boot='small', safety='low', acceptability='unacc')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = sqlContext.createDataFrame(df)\n",
    "spark_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buying', 'string'),\n",
       " ('maint', 'string'),\n",
       " ('doors', 'string'),\n",
       " ('persons', 'string'),\n",
       " ('lug_boot', 'string'),\n",
       " ('safety', 'string'),\n",
       " ('acceptability', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buying', 'string')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.select('buying').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'acceptability']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spark_df.dtypes[i][0] for i in range(len(spark_df.dtypes)) if spark_df.dtypes[i][1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[buying: string, maint: string, doors: string, persons: string, lug_boot: string, safety: string, acceptability: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummify the categorical variables.\n",
    "\n",
    "Use first the `StringIndexer`, then the `OneHotEncoderEstimator` to create the dummified variables. Be careful not to use one-hot encoding on the target variable (`acceptability`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buying\n",
      "maint\n",
      "doors\n",
      "persons\n",
      "lug_boot\n",
      "safety\n",
      "acceptability\n"
     ]
    }
   ],
   "source": [
    "for col in spark_df.columns:\n",
    "    print(col)\n",
    "    string_indexer = StringIndexer(\n",
    "        inputCol = col,\n",
    "        outputCol = col + \"_index\"\n",
    "    )\n",
    "    spark_df = string_indexer.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+-------------+--------------+------------+-------------------+\n",
      "|buying_index|maint_index|doors_index|persons_index|lug_boot_index|safety_index|acceptability_index|\n",
      "+------------+-----------+-----------+-------------+--------------+------------+-------------------+\n",
      "|         2.0|        2.0|        2.0|          2.0|           2.0|         1.0|                0.0|\n",
      "|         2.0|        2.0|        2.0|          2.0|           2.0|         0.0|                0.0|\n",
      "+------------+-----------+-----------+-------------+--------------+------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_indexed = spark_df.select([col for col in spark_df.columns if 'index' in col])\n",
    "spark_df_indexed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'acceptability_index'\n",
    "feature_columns = [col for col in spark_df_indexed.columns if col!=label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in feature_columns:\n",
    "    onehot = OneHotEncoderEstimator(\n",
    "        dropLast=True,\n",
    "        inputCols=[col],\n",
    "        outputCols=[col + \"_1\"]\n",
    "    )\n",
    "    spark_df_indexed = onehot.fit(spark_df_indexed).transform(spark_df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+-------------+--------------+------------+-------------------+--------------+-------------+-------------+---------------+----------------+--------------+\n",
      "|buying_index|maint_index|doors_index|persons_index|lug_boot_index|safety_index|acceptability_index|buying_index_1|maint_index_1|doors_index_1|persons_index_1|lug_boot_index_1|safety_index_1|\n",
      "+------------+-----------+-----------+-------------+--------------+------------+-------------------+--------------+-------------+-------------+---------------+----------------+--------------+\n",
      "|         2.0|        2.0|        2.0|          2.0|           2.0|         1.0|                0.0| (3,[2],[1.0])|(3,[2],[1.0])|(3,[2],[1.0])|      (2,[],[])|       (2,[],[])| (2,[1],[1.0])|\n",
      "|         2.0|        2.0|        2.0|          2.0|           2.0|         0.0|                0.0| (3,[2],[1.0])|(3,[2],[1.0])|(3,[2],[1.0])|      (2,[],[])|       (2,[],[])| (2,[0],[1.0])|\n",
      "+------------+-----------+-----------+-------------+--------------+------------+-------------------+--------------+-------------+-------------+---------------+----------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_indexed.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your feature columns with `VectorAssembler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in spark_df_indexed.columns if '1' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(buying_index=2.0, maint_index=2.0, doors_index=2.0, persons_index=2.0, lug_boot_index=2.0, safety_index=1.0, acceptability_index=0.0, buying_index_1=SparseVector(3, {2: 1.0}), maint_index_1=SparseVector(3, {2: 1.0}), doors_index_1=SparseVector(3, {2: 1.0}), persons_index_1=SparseVector(2, {}), lug_boot_index_1=SparseVector(2, {}), safety_index_1=SparseVector(2, {1: 1.0}), features=SparseVector(15, {2: 1.0, 5: 1.0, 8: 1.0, 14: 1.0}))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=feature_columns,\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "vector_df = vectorAssembler.transform(spark_df_indexed)\n",
    "\n",
    "vector_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(15,[2,5,8,14],[1...|\n",
      "|(15,[2,5,8,13],[1...|\n",
      "|(15,[2,5,8],[1.0,...|\n",
      "|(15,[2,5,8,11,14]...|\n",
      "|(15,[2,5,8,11,13]...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select('features').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate a spark decision tree model and tune with grid search\n",
    "\n",
    "Once done, try also other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(featuresCol='features',\n",
    "                           labelCol=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cv scores:\n",
      "[0.7864 0.805  0.8155 0.8431 0.8537 0.8623 0.8809 0.8964]\n",
      "Best model parameters:\n",
      "{'maxDepth': 10}\n",
      "\n",
      "Best model test accuracy:\n",
      "0.9202334630350194\n"
     ]
    }
   ],
   "source": [
    "(data_train, data_test) = vector_df.randomSplit([0.7, 0.3],seed=1)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "                    predictionCol='prediction',\n",
    "                    labelCol=label_column,\n",
    "                    metricName='accuracy'\n",
    "                         )\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.maxDepth, range(3,11)) \\\n",
    "    .build()\n",
    "\n",
    "# the actual gridsearch\n",
    "crossval = CrossValidator(estimator=model,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "model_fit = crossval.fit(data_train)\n",
    "\n",
    "print('Average cv scores:')\n",
    "print(np.around(np.array(model_fit.avgMetrics),4))\n",
    "\n",
    "java_model = model_fit.bestModel._java_obj\n",
    "\n",
    "print('Best model parameters:')\n",
    "print({param.name: java_model.getOrDefault(java_model.getParam(param.name)) \n",
    "    for param in paramGrid[0]})\n",
    "print()\n",
    "#print(java_model.explainParams())\n",
    "\n",
    "predictions = model_fit.transform(data_test)\n",
    "\n",
    "print('Best model test accuracy:')\n",
    "print(evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
