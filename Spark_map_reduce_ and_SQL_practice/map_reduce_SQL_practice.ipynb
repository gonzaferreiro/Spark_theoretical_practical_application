{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Practice Spark Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, we will use Spark to dig into the Bay Area Bike Share data.**\n",
    "\n",
    "Our goal is to calculate the average number of trips per hour, using the Caltrain Station as starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warning\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzaloferreiro/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: SparkContext already exists in this scope\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = sc.textFile('./data/201508_trip_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trip ID,Duration,Start Date,Start Station,Start Terminal,End Date,End Station,End Terminal,Bike #,Subscriber Type,Zip Code',\n",
       " '913460,765,8/31/2015 23:26,Harry Bridges Plaza (Ferry Building),50,8/31/2015 23:39,San Francisco Caltrain (Townsend at 4th),70,288,Subscriber,2139']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What kind of object is the data loaded as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/201508_trip_data.csv MapPartitionsRDD[319] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is an RDD\n",
    "\n",
    "trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split csv lines\n",
    "\n",
    "In spark, we can build complex pipelines that only get executed when we ask to collect them.\n",
    "\n",
    "In a python pipeline the calculation is immediately executed, but with spark the pipeline definition and execution are separate steps.\n",
    "\n",
    "In other words, we can define the pipeline with all its steps, and only when we call `collect` will the data flow through it. In order to get familiar with this new workflow, we will start with small steps to build our pipeline.\n",
    "\n",
    "**Apply a map to trips that splits each line at commas and save that to a an RDD.**\n",
    "\n",
    "> **Hint:** if you want to check that you're doing things right, you can collect the result and display the first few lines or use the .take() method and give the number of lines as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = trips.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Trip ID',\n",
       "  'Duration',\n",
       "  'Start Date',\n",
       "  'Start Station',\n",
       "  'Start Terminal',\n",
       "  'End Date',\n",
       "  'End Station',\n",
       "  'End Terminal',\n",
       "  'Bike #',\n",
       "  'Subscriber Type',\n",
       "  'Zip Code'],\n",
       " ['913460',\n",
       "  '765',\n",
       "  '8/31/2015 23:26',\n",
       "  'Harry Bridges Plaza (Ferry Building)',\n",
       "  '50',\n",
       "  '8/31/2015 23:39',\n",
       "  'San Francisco Caltrain (Townsend at 4th)',\n",
       "  '70',\n",
       "  '288',\n",
       "  'Subscriber',\n",
       "  '2139'],\n",
       " ['913459',\n",
       "  '1036',\n",
       "  '8/31/2015 23:11',\n",
       "  'San Antonio Shopping Center',\n",
       "  '31',\n",
       "  '8/31/2015 23:28',\n",
       "  'Mountain View City Hall',\n",
       "  '27',\n",
       "  '35',\n",
       "  'Subscriber',\n",
       "  '95032'],\n",
       " ['913455',\n",
       "  '307',\n",
       "  '8/31/2015 23:13',\n",
       "  'Post at Kearny',\n",
       "  '47',\n",
       "  '8/31/2015 23:18',\n",
       "  '2nd at South Park',\n",
       "  '64',\n",
       "  '468',\n",
       "  'Subscriber',\n",
       "  '94107'],\n",
       " ['913454',\n",
       "  '409',\n",
       "  '8/31/2015 23:10',\n",
       "  'San Jose City Hall',\n",
       "  '10',\n",
       "  '8/31/2015 23:17',\n",
       "  'San Salvador at 1st',\n",
       "  '8',\n",
       "  '68',\n",
       "  'Subscriber',\n",
       "  '95113']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filter for Caltrain station\n",
    "\n",
    "In Spark we can also create filters using the `filter` method.\n",
    "\n",
    "**Select station number 70 by filtering on the 5th column.** \n",
    "\n",
    "We will do all the following analysis just on this station, which corresponds to the most popular starting point. Save this to a variable called `station_70`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_70 = trips.filter(lambda x: x[4] == '70')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['913429',\n",
       "  '902',\n",
       "  '8/31/2015 21:07',\n",
       "  'San Francisco Caltrain (Townsend at 4th)',\n",
       "  '70',\n",
       "  '8/31/2015 21:22',\n",
       "  'Broadway St at Battery St',\n",
       "  '82',\n",
       "  '501',\n",
       "  'Subscriber',\n",
       "  '94133'],\n",
       " ['913426',\n",
       "  '481',\n",
       "  '8/31/2015 21:06',\n",
       "  'San Francisco Caltrain (Townsend at 4th)',\n",
       "  '70',\n",
       "  '8/31/2015 21:14',\n",
       "  'Market at 4th',\n",
       "  '76',\n",
       "  '542',\n",
       "  'Subscriber',\n",
       "  '95054']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_70.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Trips by day - hour (mapper)\n",
    "\n",
    "Let's analyse the trips by the hour. We can do this by performing a map reduce job in Spark. First we will need to emit tuples with a count of 1 for each (date, hour) key, and then we will sum the counts by key.\n",
    "\n",
    "**Emit tuple of ((date, hour), 1), applying a map to `station_70` that extracts the relevant data from each line.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emit tuple of ((date, hour), 1)\n",
    "\n",
    "trips_by_day_hour = station_70.map(lambda x: ((x[2].split()[0],\\ # 1st position in tuple: pos[0] after split x[2]\n",
    "                                               x[2].split()[1].split(':')[0]), 1)) # 2nd pos in tuple: x[0] after double spltit. And 1 apart to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('8/31/2015', '21'), 1),\n",
       " (('8/31/2015', '21'), 1),\n",
       " (('8/31/2015', '20'), 1),\n",
       " (('8/31/2015', '19'), 1),\n",
       " (('8/31/2015', '18'), 1)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_by_day_hour.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Trips by day - hour (reducer)\n",
    "\n",
    "Use the `reduceByKey` method to obtain the number of trips per (day, hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_by_day_hour_red = trips_by_day_hour.reduceByKey(lambda x, y: x+y).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1/1/2015', '11'), 1),\n",
       " (('1/1/2015', '12'), 1),\n",
       " (('1/1/2015', '14'), 2),\n",
       " (('1/1/2015', '16'), 3),\n",
       " (('1/10/2015', '10'), 2),\n",
       " (('1/10/2015', '11'), 2),\n",
       " (('1/10/2015', '12'), 7),\n",
       " (('1/10/2015', '13'), 2),\n",
       " (('1/10/2015', '18'), 1),\n",
       " (('1/10/2015', '22'), 3)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_by_day_hour_red.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1/1/2015', '11'), 1),\n",
       " (('1/1/2015', '12'), 1),\n",
       " (('1/1/2015', '14'), 2),\n",
       " (('1/1/2015', '16'), 3),\n",
       " (('1/10/2015', '10'), 2),\n",
       " (('1/10/2015', '11'), 2),\n",
       " (('1/10/2015', '12'), 7),\n",
       " (('1/10/2015', '13'), 2),\n",
       " (('1/10/2015', '18'), 1),\n",
       " (('1/10/2015', '22'), 3)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can do the same by simply using the 'add' function from the operator library\n",
    "\n",
    "from operator import add\n",
    "\n",
    "test = trips_by_day_hour.reduceByKey(add).sortByKey()\n",
    "\n",
    "test.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Trips by hour (mapper)\n",
    "\n",
    "Let's further group the trips by hour. We'll do this with a second Map Reduce job.\n",
    "\n",
    "First we will discard the day and emit tuples of (hour, count). You can achieve this with a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emit tuple of (hour, count)\n",
    "\n",
    "trips_by_hour = trips_by_day_hour_red.map(lambda x: (int(x[0][1]), x[1])) # int in pos x[0][1] and then x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 1),\n",
       " (12, 1),\n",
       " (14, 2),\n",
       " (16, 3),\n",
       " (10, 2),\n",
       " (11, 2),\n",
       " (12, 7),\n",
       " (13, 2),\n",
       " (18, 1),\n",
       " (22, 3)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_by_hour.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Trips by hour (reducer)\n",
    "\n",
    "Now calculate the average number of trips by hour using the `combineByKey` method.\n",
    "\n",
    "> You can find a suggestion on how to do it [here](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, (389, 213)),\n",
       " (14, (287, 184)),\n",
       " (16, (807, 298)),\n",
       " (10, (1151, 306)),\n",
       " (18, (2446, 307)),\n",
       " (22, (190, 141)),\n",
       " (20, (498, 231)),\n",
       " (6, (1211, 247)),\n",
       " (8, (6551, 286)),\n",
       " (0, (77, 67)),\n",
       " (2, (3, 3)),\n",
       " (4, (2, 2)),\n",
       " (11, (590, 283)),\n",
       " (13, (299, 181)),\n",
       " (9, (3081, 319)),\n",
       " (15, (390, 221)),\n",
       " (21, (311, 190)),\n",
       " (17, (2023, 307)),\n",
       " (19, (1366, 279)),\n",
       " (7, (4497, 260)),\n",
       " (23, (88, 75)),\n",
       " (5, (39, 31)),\n",
       " (3, (1, 1)),\n",
       " (1, (7, 7))]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's find it without using the combineByKey method\n",
    "\n",
    "trips_by_day_hour_red \\\n",
    "    .map(lambda x: (int(x[0][1]),(x[1],1))) \\ # the hour first as int and in second place the occurrences and 1\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\ # the hour is the key and we're adding x and y\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_by_key = trips_by_day_hour_red \\\n",
    "    .map(lambda x: (int(x[0][1]),(x[1],1))) \\# the hour first as int and in second place the occurrences and 1\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\# the hour is the key and we're adding x and y\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\# finally we map x as x[0] / x[1]\n",
    "    .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.1492537313432836),\n",
       " (1, 1.0),\n",
       " (2, 1.0),\n",
       " (3, 1.0),\n",
       " (4, 1.0),\n",
       " (5, 1.2580645161290323),\n",
       " (6, 4.902834008097166),\n",
       " (7, 17.296153846153846),\n",
       " (8, 22.905594405594407),\n",
       " (9, 9.658307210031348),\n",
       " (10, 3.761437908496732),\n",
       " (11, 2.0848056537102475),\n",
       " (12, 1.8262910798122065),\n",
       " (13, 1.6519337016574585),\n",
       " (14, 1.559782608695652),\n",
       " (15, 1.7647058823529411),\n",
       " (16, 2.708053691275168),\n",
       " (17, 6.58957654723127),\n",
       " (18, 7.96742671009772),\n",
       " (19, 4.896057347670251),\n",
       " (20, 2.155844155844156),\n",
       " (21, 1.6368421052631579),\n",
       " (22, 1.3475177304964538),\n",
       " (23, 1.1733333333333333)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_by_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same using combineByKey\n",
    "# commbineByKey is a generic function to combine the elements for each key using a custom\n",
    "# set of aggregation functions.\n",
    "\n",
    "sumCount = trips_by_hour.combineByKey(# we'll start from the trips_by_hour\n",
    "                             lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1), \n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The combineByKey Method**\n",
    "\n",
    "In order to aggregate an RDD’s elements in parallel, Spark’s combineByKey method requires three functions:\n",
    "\n",
    "* createCombiner\n",
    "* mergeValue\n",
    "* mergeCombiner\n",
    "\n",
    "**Create a Combiner**\n",
    "\n",
    "* lambda value: (value, 1)\n",
    "\n",
    "The first required argument in the combineByKey method is a function to be used as the very first aggregation step for each key. The argument of this function corresponds to the value in a key-value pair. If we want to compute the sum and count using combineByKey, then we can create this “combiner” to be a tuple in the form of (sum, count). The very first step in this aggregation is then (value, 1), where value is the first RDD value that combineByKey comes across and 1 initializes the count.\n",
    "\n",
    "**Merge a Value**\n",
    "\n",
    "* lambda x, value: (x[0] + value, x[1] + 1)\n",
    "\n",
    "The next required function tells combineByKey what to do when a combiner is given a new value. The arguments to this function are a combiner and a new value. The structure of the combiner is defined above as a tuple in the form of (sum, count) so we merge the new value by adding it to the first element of the tuple while incrementing 1 to the second element of the tuple.\n",
    "\n",
    "**Merge two Combiners**\n",
    "\n",
    "* lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "The final required function tells combineByKey how to merge two combiners. In this example with tuples as combiners in the form of (sum, count), all we need to do is add the first and last elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, (389, 213)),\n",
       " (14, (287, 184)),\n",
       " (16, (807, 298)),\n",
       " (10, (1151, 306)),\n",
       " (18, (2446, 307)),\n",
       " (22, (190, 141)),\n",
       " (20, (498, 231)),\n",
       " (6, (1211, 247)),\n",
       " (8, (6551, 286)),\n",
       " (0, (77, 67)),\n",
       " (2, (3, 3)),\n",
       " (4, (2, 2)),\n",
       " (11, (590, 283)),\n",
       " (13, (299, 181)),\n",
       " (9, (3081, 319)),\n",
       " (15, (390, 221)),\n",
       " (21, (311, 190)),\n",
       " (17, (2023, 307)),\n",
       " (19, (1366, 279)),\n",
       " (7, (4497, 260)),\n",
       " (23, (88, 75)),\n",
       " (5, (39, 31)),\n",
       " (3, (1, 1)),\n",
       " (1, (7, 7))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageByKey = sumCount.map(lambda x: (x[0], x[1][0] / x[1][1] )).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.1492537313432836),\n",
       " (1, 1.0),\n",
       " (2, 1.0),\n",
       " (3, 1.0),\n",
       " (4, 1.0),\n",
       " (5, 1.2580645161290323),\n",
       " (6, 4.902834008097166),\n",
       " (7, 17.296153846153846),\n",
       " (8, 22.905594405594407),\n",
       " (9, 9.658307210031348),\n",
       " (10, 3.761437908496732),\n",
       " (11, 2.0848056537102475),\n",
       " (12, 1.8262910798122065),\n",
       " (13, 1.6519337016574585),\n",
       " (14, 1.559782608695652),\n",
       " (15, 1.7647058823529411),\n",
       " (16, 2.708053691275168),\n",
       " (17, 6.58957654723127),\n",
       " (18, 7.96742671009772),\n",
       " (19, 4.896057347670251),\n",
       " (20, 2.155844155844156),\n",
       " (21, 1.6368421052631579),\n",
       " (22, 1.3475177304964538),\n",
       " (23, 1.1733333333333333)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByKey.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Using the Spark `sqlContext`\n",
    "\n",
    "Besides the SparkContext, Spark also exposes a sqlContext that allows us to perform SQL queries on an RDD object.**We'll run a query using a sqlContext to obtain the average duration of a trip originating from the Caltrain station.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x1184415c0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripsSql = sqlContext.read.format('com.databricks.spark.csv').options(header='true',\n",
    "                                                                      inferschema='true').load('./data/201508_trip_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "|Trip ID|Duration|     Start Date|       Start Station|Start Terminal|       End Date|         End Station|End Terminal|Bike #|Subscriber Type|Zip Code|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "| 913460|     765|8/31/2015 23:26|Harry Bridges Pla...|            50|8/31/2015 23:39|San Francisco Cal...|          70|   288|     Subscriber|    2139|\n",
      "| 913459|    1036|8/31/2015 23:11|San Antonio Shopp...|            31|8/31/2015 23:28|Mountain View Cit...|          27|    35|     Subscriber|   95032|\n",
      "| 913455|     307|8/31/2015 23:13|      Post at Kearny|            47|8/31/2015 23:18|   2nd at South Park|          64|   468|     Subscriber|   94107|\n",
      "| 913454|     409|8/31/2015 23:10|  San Jose City Hall|            10|8/31/2015 23:17| San Salvador at 1st|           8|    68|     Subscriber|   95113|\n",
      "| 913453|     789|8/31/2015 23:09|Embarcadero at Fo...|            51|8/31/2015 23:22|Embarcadero at Sa...|          60|   487|       Customer|    9069|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tripsSql.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: integer (nullable = true)\n",
      " |-- Duration: integer (nullable = true)\n",
      " |-- Start Date: string (nullable = true)\n",
      " |-- Start Station: string (nullable = true)\n",
      " |-- Start Terminal: integer (nullable = true)\n",
      " |-- End Date: string (nullable = true)\n",
      " |-- End Station: string (nullable = true)\n",
      " |-- End Terminal: integer (nullable = true)\n",
      " |-- Bike #: integer (nullable = true)\n",
      " |-- Subscriber Type: string (nullable = true)\n",
      " |-- Zip Code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tripsSql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trip ID',\n",
       " 'Duration',\n",
       " 'Start Date',\n",
       " 'Start Station',\n",
       " 'Start Terminal',\n",
       " 'End Date',\n",
       " 'End Station',\n",
       " 'End Terminal',\n",
       " 'Bike #',\n",
       " 'Subscriber Type',\n",
       " 'Zip Code']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripsSql.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Trip ID=913460)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripsSql.select(col('Trip ID')).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns to be able to use SQL\n",
    "df = tripsSql.select(col(\"Trip ID\").alias('Trip_ID'),\n",
    "                     col(\"Duration\"),\n",
    "                     col(\"Start Date\").alias('Start_Date'),\n",
    "                     col(\"Start Station\").alias('Start_Station'),\n",
    "                     col(\"Start Terminal\").alias('Start_Terminal'),\n",
    "                     col(\"End Date\").alias('End_Date'),\n",
    "                     col(\"End Station\").alias('End_Station'),\n",
    "                     col(\"End Terminal\").alias('End_Terminal'),\n",
    "                     col(\"Bike #\").alias('Bike_id'),\n",
    "                     col(\"Subscriber Type\").alias('Subscriber_Type'),\n",
    "                     col(\"Zip Code\").alias('Zip_Code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register this DataFrame as a table.\n",
    "df.registerTempTable(\"tripsSql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|     Start_Date|\n",
      "+---------------+\n",
      "|8/31/2015 23:26|\n",
      "|8/31/2015 23:11|\n",
      "|8/31/2015 23:13|\n",
      "|8/31/2015 23:10|\n",
      "|8/31/2015 23:09|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "SELECT t.Start_Date \n",
    "FROM tripsSql t\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|hour|duration|\n",
      "+----+--------+\n",
      "|  21|     902|\n",
      "|  21|     481|\n",
      "|  20|     842|\n",
      "|  19|     259|\n",
      "|  18|     226|\n",
      "|  18|     769|\n",
      "|  18|     727|\n",
      "|  18|     756|\n",
      "|  18|     515|\n",
      "|  18|    1280|\n",
      "|  18|    1139|\n",
      "|  18|     702|\n",
      "|  17|     926|\n",
      "|  17|     696|\n",
      "|  17|     908|\n",
      "|  17|     919|\n",
      "|  17|     718|\n",
      "|  17|     507|\n",
      "|  17|     663|\n",
      "|  16|     818|\n",
      "|  16|     553|\n",
      "|  16|     458|\n",
      "|  15|     656|\n",
      "|  15|     611|\n",
      "+----+--------+\n",
      "only showing top 24 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "SELECT\n",
    "    CAST(SPLIT(SPLIT(t.Start_Date, ' ')[1], ':')[0] AS INT) AS hour,\n",
    "    t.Duration AS duration\n",
    "FROM tripsSql t\n",
    "WHERE\n",
    "    t.Start_Terminal = 70\n",
    "    AND\n",
    "    t.Duration IS NOT NULL\n",
    "\"\"\").show(n=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------+\n",
      "|hour|   c|avg_duration|\n",
      "+----+----+------------+\n",
      "|   0|  77|        45.0|\n",
      "|   1|   7|        15.0|\n",
      "|   2|   3|        13.0|\n",
      "|   3|   1|         4.0|\n",
      "|   4|   2|       491.0|\n",
      "|   5|  39|        38.0|\n",
      "|   6|1211|        12.0|\n",
      "|   7|4497|        13.0|\n",
      "|   8|6551|        12.0|\n",
      "|   9|3081|        12.0|\n",
      "|  10|1151|        15.0|\n",
      "|  11| 590|        24.0|\n",
      "|  12| 389|        41.0|\n",
      "|  13| 299|        42.0|\n",
      "|  14| 287|        24.0|\n",
      "|  15| 390|        15.0|\n",
      "|  16| 807|        17.0|\n",
      "|  17|2023|        12.0|\n",
      "|  18|2446|        12.0|\n",
      "|  19|1366|        11.0|\n",
      "|  20| 498|        14.0|\n",
      "|  21| 311|        12.0|\n",
      "|  22| 190|        17.0|\n",
      "|  23|  88|        11.0|\n",
      "+----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "SELECT\n",
    "    hour,\n",
    "    COUNT(1) AS c,\n",
    "    ROUND(AVG(duration) / 60) AS avg_duration\n",
    "FROM (\n",
    "    SELECT\n",
    "        CAST(SPLIT(SPLIT(t.Start_Date, ' ')[1], ':')[0] AS INT) AS hour,\n",
    "        t.Duration AS duration\n",
    "    FROM tripsSql t\n",
    "    WHERE\n",
    "        t.Start_Terminal = 70\n",
    "        AND\n",
    "        t.Duration IS NOT NULL\n",
    "    ) r\n",
    "GROUP BY hour\n",
    "ORDER BY hour ASC\n",
    "\"\"\").show(n=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average counts per hour and day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|        avg(count)|\n",
      "+----+------------------+\n",
      "|   0|1.1492537313432836|\n",
      "|   1|               1.0|\n",
      "|   2|               1.0|\n",
      "|   3|               1.0|\n",
      "|   4|               1.0|\n",
      "|   5|1.2580645161290323|\n",
      "|   6| 4.902834008097166|\n",
      "|   7|17.296153846153846|\n",
      "|   8|22.905594405594407|\n",
      "|   9| 9.658307210031348|\n",
      "|  10| 3.761437908496732|\n",
      "|  11|2.0848056537102475|\n",
      "|  12|1.8262910798122065|\n",
      "|  13|1.6519337016574585|\n",
      "|  14| 1.559782608695652|\n",
      "|  15|1.7647058823529411|\n",
      "|  16| 2.708053691275168|\n",
      "|  17|  6.58957654723127|\n",
      "|  18|  7.96742671009772|\n",
      "|  19| 4.896057347670251|\n",
      "|  20| 2.155844155844156|\n",
      "|  21|1.6368421052631579|\n",
      "|  22|1.3475177304964538|\n",
      "|  23|1.1733333333333333|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    select hour, avg(count) from (\n",
    "    select hour, day, count(day) as count from \n",
    "    (SELECT\n",
    "        SPLIT(t.Start_Date, ' ')[0] as day,\n",
    "        CAST(SPLIT(SPLIT(t.Start_Date, ' ')[1], ':')[0] AS INT) AS hour\n",
    "    FROM tripsSql t\n",
    "    WHERE\n",
    "        t.Start_Terminal = 70\n",
    "        AND\n",
    "        t.Duration IS NOT NULL) as s\n",
    "    group by hour, day\n",
    "    order by hour, day) as r\n",
    "    group by hour\n",
    "    order by hour\n",
    "\"\"\").show(24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
